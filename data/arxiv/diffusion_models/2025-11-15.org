#+TITLE: ARXIV 爬取结果 - diffusion - 2025-11-15
#+DATE: 2025-11-15
#+AUTHOR: Org Crawler
#+CATEGORY: diffusion
#+CREATED: 2025-11-15 18:19:30

* [[https://arxiv.org/abs/2511.10101][Takehiro Ishikawa --- Balancing Centralized Learning and Distributed Self-Organization: A Hybrid Model for Embodied Morphogenesis]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10101
:ID: oai:arXiv.org:2511.10101v1
:ARXIV_ID: 2511.10101
:CATEGORIES: cs.AI
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: new
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
Balancing Centralized Learning and Distributed Self-Organization: A Hybrid Model for Embodied Morphogenesis

** 链接
[[https://arxiv.org/abs/2511.10101][https://arxiv.org/abs/2511.10101]]

** 摘要
We investigate how to couple a learnable brain-like'' controller to a cell-like'' Gray--Scott substrate to steer pattern formation with minimal effort. A compact convolutional policy is embedded in a differentiable PyTorch reaction--diffusion simulator, producing spatially smooth, bounded modulations of the feed and kill parameters ($\Delta F$, $\Delta K$) under a warm--hold--decay gain schedule. Training optimizes Turing-band spectral targets (FFT-based) while penalizing control effort ($\ell_1/\ell_2$) and instability. We compare three regimes: pure reaction--diffusion, NN-dominant, and a hybrid coupling. The hybrid achieves reliable, fast formation of target textures: 100% strict convergence in $\sim 165$ steps, matching cell-only spectral selectivity (0.436 vs.\ 0.434) while using $\sim 15\times$ less $\ell_1$ effort and $>200\times$ less $\ell_2$ power than NN-dominant control. An amplitude sweep reveals a non-monotonic Goldilocks'' zone ($A \approx 0.03$--$0.045$) that yields 100\% quasi convergence in 94--96 steps, whereas weaker or stronger gains fail to converge or degrade selectivity. These results quantify morphological computation: the controller seeds then cedes,'' providing brief, sparse nudges that place the system in the correct basin of attraction, after which local physics maintains the pattern. The study offers a practical recipe for building steerable, robust, and energy-efficient embodied systems that exploit an optimal division of labor between centralized learning and distributed self-organization.

** 作者
Takehiro Ishikawa

** 关键词
diffusion


* [[https://arxiv.org/abs/2511.09568][Peining Zhang --- VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.09568
:ID: oai:arXiv.org:2511.09568v1
:ARXIV_ID: 2511.09568
:CATEGORIES: physics.chem-ph, cs.AI, cs.CV
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing

** 链接
[[https://arxiv.org/abs/2511.09568][https://arxiv.org/abs/2511.09568]]

** 摘要
Diffusion models show promise for 3D molecular generation, but face a fundamental trade-off between sampling efficiency and conformational accuracy. While flow-based models are fast, they often produce geometrically inaccurate structures, as they have difficulty capturing the multimodal distributions of molecular conformations. In contrast, denoising diffusion models are more accurate but suffer from slow sampling, a limitation attributed to sub-optimal integration between diffusion dynamics and SE(3)-equivariant architectures. To address this, we propose VEDA, a unified SE(3)-equivariant framework that combines variance-exploding diffusion with annealing to efficiently generate conformationally accurate 3D molecular structures. Specifically, our key technical contributions include: (1) a VE schedule that enables noise injection functionally analogous to simulated annealing, improving 3D accuracy and reducing relaxation energy; (2) a novel preconditioning scheme that reconciles the coordinate-predicting nature of SE(3)-equivariant networks with a residual-based diffusion objective, and (3) a new arcsin-based scheduler that concentrates sampling in critical intervals of the logarithmic signal-to-noise ratio. On the QM9 and GEOM-DRUGS datasets, VEDA matches the sampling efficiency of flow-based models, achieving state-of-the-art valency stability and validity with only 100 sampling steps. More importantly, VEDA's generated structures are remarkably stable, as measured by their relaxation energy during GFN2-xTB optimization. The median energy change is only 1.72 kcal/mol, significantly lower than the 32.3 kcal/mol from its architectural baseline, SemlaFlow. Our framework demonstrates that principled integration of VE diffusion with SE(3)-equivariant architectures can achieve both high chemical accuracy and computational efficiency.

** 作者
Peining Zhang, Jinbo Bi, Minghu Song

** 关键词
diffusion model, diffusion, diffusion models


* [[https://arxiv.org/abs/2511.09578][Hadi Keramati --- HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.09578
:ID: oai:arXiv.org:2511.09578v1
:ARXIV_ID: 2511.09578
:CATEGORIES: cs.LG, cs.AI, physics.comp-ph
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization

** 链接
[[https://arxiv.org/abs/2511.09578][https://arxiv.org/abs/2511.09578]]

** 摘要
This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.

** 作者
Hadi Keramati, Morteza Sadeghi, Rajeev K. Jaiman

** 关键词
diffusion model, diffusion


* [[https://arxiv.org/abs/2511.09895][Xiaoda Wang --- Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.09895
:ID: oai:arXiv.org:2511.09895v1
:ARXIV_ID: 2511.09895
:CATEGORIES: cs.LG, cs.AI
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation

** 链接
[[https://arxiv.org/abs/2511.09895][https://arxiv.org/abs/2511.09895]]

** 摘要
Cardiovascular disease (CVD) is a leading cause of mortality worldwide. Electrocardiograms (ECGs) are the most widely used non-invasive tool for cardiac assessment, yet large, well-annotated ECG corpora are scarce due to cost, privacy, and workflow constraints. Generating ECGs can be beneficial for the mechanistic understanding of cardiac electrical activity, enable the construction of large, heterogeneous, and unbiased datasets, and facilitate privacy-preserving data sharing. Generating realistic ECG signals from clinical context is important yet underexplored. Recent work has leveraged diffusion models for text-to-ECG generation, but two challenges remain: (i) existing methods often overlook the physiological simulator knowledge of cardiac activity; and (ii) they ignore broader, experience-based clinical knowledge grounded in real-world practice. To address these gaps, we propose SE-Diff, a novel physiological simulator and experience enhanced diffusion model for comprehensive ECG generation. SE-Diff integrates a lightweight ordinary differential equation (ODE)-based ECG simulator into the diffusion process via a beat decoder and simulator-consistent constraints, injecting mechanistic priors that promote physiologically plausible waveforms. In parallel, we design an LLM-powered experience retrieval-augmented strategy to inject clinical knowledge, providing more guidance for ECG generation. Extensive experiments on real-world ECG datasets demonstrate that SE-Diff improves both signal fidelity and text-ECG semantic alignment over baselines, proving its superiority for text-to-ECG generation. We further show that the simulator-based and experience-based knowledge also benefit downstream ECG classification.

** 作者
Xiaoda Wang, Kaiqiao Han, Yuhao Xu, Xiao Luo, Yizhou Sun, Wei Wang, Carl Yang

** 关键词
diffusion model, diffusion, diffusion models


* [[https://arxiv.org/abs/2511.09962][Ziqing Yin --- AI-Integrated Decision Support System for Real-Time Market Growth Forecasting and Multi-Source Content Diffusion Analytics]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.09962
:ID: oai:arXiv.org:2511.09962v1
:ARXIV_ID: 2511.09962
:CATEGORIES: cs.LG, cs.AI, cs.MM
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
AI-Integrated Decision Support System for Real-Time Market Growth Forecasting and Multi-Source Content Diffusion Analytics

** 链接
[[https://arxiv.org/abs/2511.09962][https://arxiv.org/abs/2511.09962]]

** 摘要
The rapid proliferation of AI-generated content (AIGC) has reshaped the dynamics of digital marketing and online consumer behavior. However, predicting the diffusion trajectory and market impact of such content remains challenging due to data heterogeneity, non linear propagation mechanisms, and evolving consumer interactions. This study proposes an AI driven Decision Support System (DSS) that integrates multi source data including social media streams, marketing expenditure records, consumer engagement logs, and sentiment dynamics using a hybrid Graph Neural Network (GNN) and Temporal Transformer framework. The model jointly learns the content diffusion structure and temporal influence evolution through a dual channel architecture, while causal inference modules disentangle the effects of marketing stimuli on return on investment (ROI) and market visibility. Experiments on large scale real-world datasets collected from multiple online platforms such as Twitter, TikTok, and YouTube advertising show that our system outperforms existing baselines in all six metrics. The proposed DSS enhances marketing decisions by providing interpretable real-time insights into AIGC driven content dissemination and market growth patterns.

** 作者
Ziqing Yin, Xuanjing Chen, Xi Zhang

** 关键词
diffusion


* [[https://arxiv.org/abs/2511.10087][Haidong Huang --- Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10087
:ID: oai:arXiv.org:2511.10087v1
:ARXIV_ID: 2511.10087
:CATEGORIES: cs.RO, cs.AI, cs.LG
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning

** 链接
[[https://arxiv.org/abs/2511.10087][https://arxiv.org/abs/2511.10087]]

** 摘要
Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\% on dexterous manipulation, demonstrating strong generalization and scalability.

** 作者
Haidong Huang, Haiyue Zhu. Jiayu Song, Xixin Zhao, Yaohua Zhou, Jiayi Zhang, Yuze Zhai, Xiaocong Li

** 关键词
diffusion


* [[https://arxiv.org/abs/2511.10154][Hao Zou --- GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10154
:ID: oai:arXiv.org:2511.10154v1
:ARXIV_ID: 2511.10154
:CATEGORIES: cs.CV, cs.AI
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval

** 链接
[[https://arxiv.org/abs/2511.10154][https://arxiv.org/abs/2511.10154]]

** 摘要
Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at https://github.com/sugelamyd123/Sup-for-GEA.

** 作者
Hao Zou, Runqing Zhang, Xue Zhou, Jianxiao Zou

** 关键词
diffusion


* [[https://arxiv.org/abs/2511.10208][Cheng Kevin Qu --- Fractional neural attention for efficient multiscale sequence processing]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10208
:ID: oai:arXiv.org:2511.10208v1
:ARXIV_ID: 2511.10208
:CATEGORIES: cs.LG, cs.AI, math.DS, math.PR, physics.bio-ph
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
Fractional neural attention for efficient multiscale sequence processing

** 链接
[[https://arxiv.org/abs/2511.10208][https://arxiv.org/abs/2511.10208]]

** 摘要
Attention mechanisms underpin the computational power of Transformer models, which have achieved remarkable success across diverse domains. Yet understanding and extending the principles underlying self-attention remains a key challenge for advancing artificial intelligence. Drawing inspiration from the multiscale dynamics of biological attention and from dynamical systems theory, we introduce Fractional Neural Attention (FNA), a principled, neuroscience-inspired framework for multiscale information processing. FNA models token interactions through L\'evy diffusion governed by the fractional Laplacian, intrinsically realizing simultaneous short- and long-range dependencies across multiple scales. This mechanism yields greater expressivity and faster information mixing, advancing the foundational capacity of Transformers. Theoretically, we show that FNA's dynamics are governed by the fractional diffusion equation, and that the resulting attention networks exhibit larger spectral gaps and shorter path lengths -- mechanistic signatures of enhanced computational efficiency. Empirically, FNA achieves competitive text-classification performance even with a single layer and a single head; it also improves performance in image processing and neural machine translation. Finally, the diffusion map algorithm from geometric harmonics enables dimensionality reduction of FNA weights while preserving the intrinsic structure of embeddings and hidden states. Together, these results establish FNA as a principled mechanism connecting self-attention, stochastic dynamics, and geometry, providing an interpretable, biologically grounded foundation for powerful, neuroscience-inspired AI.

** 作者
Cheng Kevin Qu, Andrew Ly, Pulin Gong

** 关键词
diffusion


* [[https://arxiv.org/abs/2511.10384][Raj Gaurav Maurya --- Simulating Misinformation Propagation in Social Networks using Large Language Models]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10384
:ID: oai:arXiv.org:2511.10384v1
:ARXIV_ID: 2511.10384
:CATEGORIES: cs.SI, cs.AI, cs.CL, cs.CY
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
Simulating Misinformation Propagation in Social Networks using Large Language Models

** 链接
[[https://arxiv.org/abs/2511.10384][https://arxiv.org/abs/2511.10384]]

** 摘要
Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.

** 作者
Raj Gaurav Maurya, Vaibhav Shukla, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat

** 关键词
diffusion, agent, agents


* [[https://arxiv.org/abs/2511.10403][Mingxing Peng --- nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10403
:ID: oai:arXiv.org:2511.10403v1
:ARXIV_ID: 2511.10403
:CATEGORIES: cs.RO, cs.AI
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation

** 链接
[[https://arxiv.org/abs/2511.10403][https://arxiv.org/abs/2511.10403]]

** 摘要
Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.

** 作者
Mingxing Peng, Ruoyu Yao, Xusen Guo, Jun Ma

** 关键词
diffusion, agent, agents, multi-agent


* [[https://arxiv.org/abs/2511.10555][Huijie Liu --- A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.10555
:ID: oai:arXiv.org:2511.10555v1
:ARXIV_ID: 2511.10555
:CATEGORIES: cs.CV, cs.AI
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space

** 链接
[[https://arxiv.org/abs/2511.10555][https://arxiv.org/abs/2511.10555]]

** 摘要
Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.

** 作者
Huijie Liu, Shuhao Cui, Haoxiang Cao, Shuai Ma, Kai Wu, Guoliang Kang

** 关键词
diffusion model, diffusion


* [[https://arxiv.org/abs/2509.05314][Ying Li --- ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2509.05314
:ID: oai:arXiv.org:2509.05314v2
:ARXIV_ID: 2509.05314
:CATEGORIES: cs.RO, cs.AI, cs.CV
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: replace-cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory

** 链接
[[https://arxiv.org/abs/2509.05314][https://arxiv.org/abs/2509.05314]]

** 摘要
Data scarcity continues to be a major challenge in the field of robotic manipulation. Although diffusion models provide a promising solution for generating robotic manipulation videos, existing methods largely depend on 2D trajectories, which inherently face issues with 3D spatial ambiguity. In this work, we present a novel framework named ManipDreamer3D for generating plausible 3D-aware robotic manipulation videos from the input image and the text instruction. Our method combines 3D trajectory planning with a reconstructed 3D occupancy map created from a third-person perspective, along with a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D first reconstructs the 3D occupancy representation from the input image and then computes an optimized 3D end-effector trajectory, minimizing path length while avoiding collisions. Next, we employ a latent editing technique to create video sequences from the initial image latent and the optimized 3D trajectory. This process conditions our specially trained trajectory-to-video diffusion model to produce robotic pick-and-place videos. Our method generates robotic videos with autonomously planned plausible 3D trajectories, significantly reducing human intervention requirements. Experimental results demonstrate superior visual quality compared to existing methods.

** 作者
Ying Li, Xiaobao Wei, Xiaowei Chi, Yuming Li, Zhongyu Zhao, Hao Wang, Ningning Ma, Ming Lu, Sirui Han, Shanghang Zhang

** 关键词
diffusion model, diffusion, diffusion models


* [[https://arxiv.org/abs/2511.09057][PAN Team --- PAN: A World Model for General, Interactable, and Long-Horizon World Simulation]]
:PROPERTIES:
:URL: https://arxiv.org/abs/2511.09057
:ID: oai:arXiv.org:2511.09057v2
:ARXIV_ID: 2511.09057
:CATEGORIES: cs.CV, cs.AI, cs.CL, cs.LG
:PUBLISHED_TIME: 2025-11-15T05:00:00
:ARXIV_ANNOUNCE_TYPE: replace-cross
:CRAWL_TIME: 2025-11-15 18:19:30

:END:

** 标题
PAN: A World Model for General, Interactable, and Long-Horizon World Simulation

** 链接
[[https://arxiv.org/abs/2511.09057][https://arxiv.org/abs/2511.09057]]

** 摘要
A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

** 作者
PAN Team, Jiannan Xiang, Yi Gu, Zihan Liu, Zeyu Feng, Qiyue Gao, Yiyan Hu, Benhao Huang, Guangyi Liu, Yichi Yang, Kun Zhou, Davit Abrahamyan, Arif Ahmad, Ganesh Bannur, Junrong Chen, Kimi Chen, Mingkai Deng, Ruobing Han, Xinqi Huang, Haoqiang Kang, Zheqi Li, Enze Ma, Hector Ren, Yashowardhan Shinde, Rohan Shingre, Ramsundar Tanikella, Kaiming Tao, Dequan Yang, Xinle Yu, Cong Zeng, Binglin Zhou, Zhengzhong Liu, Zhiting Hu, Eric P. Xing

** 关键词
diffusion, agent

